
# üöÄ Jetson Copilot V4.1

**Fully Modernized, Jetson-Optimized Ollama + Streamlit AI Assistant**

---

## ‚úÖ Quick Start

### 1Ô∏è‚É£ Clone and enter project

```bash
git clone <your-repo-url> jetson-copilot-v4.1
cd jetson-copilot-v4.1
```

### 2Ô∏è‚É£ Install NVIDIA Container Runtime (Jetson Required)

Make sure you have JetPack SDK installed on your Jetson device.

```bash
sudo apt install nvidia-container-toolkit
sudo systemctl restart docker
```

### 3Ô∏è‚É£ Build Docker Image

```bash
./build_copilot.sh
```

### 4Ô∏è‚É£ Run the Copilot

```bash
./run_copilot.sh
```

---

## ‚úÖ Features

- ‚úÖ ARM64v8 fully compatible (Jetson Nano, Xavier, Orin etc)
- ‚úÖ Streamlit 1.35+ UI
- ‚úÖ Ollama API integration (v0.5.1+)
- ‚úÖ LlamaIndex 0.12+ modernized RAG pipeline
- ‚úÖ Streaming response buffering for smoother UI
- ‚úÖ Dynamic document uploading (PDF, DOCX, Markdown, TXT)
- ‚úÖ Full RAG Indexing (VectorStoreIndex with SentenceSplitter)

---

## ‚úÖ Requirements

- Jetson device with JetPack SDK installed
- Docker with NVIDIA runtime enabled
- External Ollama model storage (`~/.ollama`) mounted on host for model access

---

## ‚úÖ Models Used

- `llama3:latest`  ‚Üí Chat LLM (ensure downloaded in Ollama)
- `mxbai-embed-large:latest`  ‚Üí Embedding model

---

## ‚úÖ Notes

- All uploaded files are temporarily indexed inside container memory unless persistent indexes are implemented.
- If you wish to mount your existing `~/.ollama` model cache into Docker, add this volume:

```bash
-v $HOME/.ollama:/root/.ollama
```

---

Built for production Jetson deployment.
