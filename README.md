# ğŸš€ Jetson Copilot V3.1.2

A fully local, GPU-accelerated, hybrid-RAG AI assistant for NVIDIA Jetson AGX Orin (and beyond).

---

## ğŸ”¥ Features

- Fully self-hosted AI chat engine powered by Ollama
- Supports local model inference with streaming chat
- Hybrid RAG mode combining indexed documents + live uploaded files
- Model catalog intelligence powered by GPT for easy model evaluation
- Supports PDFs, DOCX, Markdown, TXT ingestion
- ARM64-native optimized build for Jetson Orin

---

## âš™ Architecture

| Layer | Location |
|-------|----------|
| Ollama Server | Installed directly on Jetson host (ARM64 binary) |
| Ollama Model Files | `~/.ollama` on host |
| Streamlit App (Copilot) | Inside Docker container |
| Ollama Python Client (v0.5.1) | Inside Docker |
| LlamaIndex (v0.10.20 stack) | Inside Docker |
| Document Storage | Mounted index directory inside Docker |

---

## ğŸš€ Build Instructions (Jetson Copilot V3.1.2 Optimized Build)

### 1ï¸âƒ£ Host dependencies (Jetson or DGX)

- Host Ollama server installed at version >= 0.1.27+
- Ollama ARM64 binary installed directly on host

Verify Ollama version:

```bash
ollama --version
# Should report: v0.1.27 (or newer)
You can safely upgrade Ollama on host by downloading the ARM64 binary from:
https://github.com/ollama/ollama/releases
```

2ï¸âƒ£ Clone repository & build docker image
bash
Copy
Edit
git clone https://github.com/YOUR_REPO/jetson-copilot-clean
cd jetson-copilot-clean

# Build optimized Docker image
./build_copilot.sh  
âœ… The first build may take several minutes  
âœ… Subsequent rebuilds will be dramatically faster  

3ï¸âƒ£ Run Jetson Copilot container  
```bash  
./run_copilot.sh
By default, Streamlit app will be served on:
http://localhost:8501/
```
The app inside Docker connects to Ollama running on the Jetson host using http://127.0.0.1:11434

ğŸ§¬ Build System Optimization
âš¡ Fully cached pip install layers (requirements.txt)  
âš¡ App code changes no longer invalidate package layers  
âš¡ Future rebuilds typically complete in ~30-60 seconds  
ğŸ“ Model Download  
Use the built-in model downloader inside the Streamlit app:  
Supports automatic pulling of models from Ollama library  
GPT-powered enrichment metadata automatically updates model catalog  
Jetson-safety indicators assist with proper model selection

ğŸ”§ File Structure
```bash
jetson-copilot-clean/
â”‚
â”œâ”€â”€ Dockerfile             # Fully optimized V3.1.2 build file
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ build_copilot.sh       # Build script
â”œâ”€â”€ run_copilot.sh         # Launch script
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py             # Main Streamlit app
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ func.py        # Utility functions
â”‚   â”‚   â”œâ”€â”€ constants.py   # App constants
â”‚   â”‚   â””â”€â”€ model_catalog_utils.py  # GPT-powered model enrichment logic
â”‚   â””â”€â”€ pages/
â”‚       â””â”€â”€ download_model.py   # Ollama model download interface
â””â”€â”€ Indexes/               # Document vector storage
```

ğŸ’¡ Roadmap  
âœ… V3.1.2: Optimized build system  
ğŸ”„ V3.2.x: Optional multi-agent extensions  
ğŸ”„ V3.3.x: Cluster orchestration & distributed Ollama support

ğŸ’– Credits
NVIDIA Jetson AGX Orin  
Ollama model server  
LlamaIndex  
Streamlit