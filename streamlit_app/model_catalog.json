{
  "llama3:latest": {
    "RAM": "~8-10 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… Jetson 16GB+ recommended",
    "Why": "Best default Llama3, fits AGX, great general assistant"
  },
  "llama3.1:latest": {
    "RAM": "~10-12 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… Jetson 32GB+ recommended",
    "Why": "Llama3.1, best performance per size, solid for chat/coding"
  },
  "llama3.1:8b": {
    "RAM": "28-32 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âŒ borderline unless you have full 64GB AGX",
    "Why": "Full 8B model, max accuracy, big RAM only"
  },
  "llama4:latest": {
    "RAM": ">=32 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âŒ Only for top-end DGX/desktop",
    "Why": "Llama 4 (full), advanced reasoning, huge memory"
  },
  "llama4:maverick": {
    "RAM": "~4 GB",
    "Reasoning": "ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… always safe",
    "Why": "Ultra-light Llama 4 demo, runs anywhere"
  },
  "dolphin3:8b": {
    "RAM": "~8-12 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… AGX 32GB+",
    "Why": "Open Hermes Dolphin, creative and chatty"
  },
  "gemma3:4b": {
    "RAM": "~7-8 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… AGX 16GB+",
    "Why": "Gemma 3B, solid factual accuracy, Google research"
  },
  "deepseek-r1:8b": {
    "RAM": "~12 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… AGX 32GB+",
    "Why": "DeepSeek R1, excels at search and retrieval"
  },
  "mistral:7b": {
    "RAM": "~8-10 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… safe",
    "Why": "Excellent instruction following, small but smart"
  },
  "codellama:7b-instruct": {
    "RAM": "~8-10 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… safe",
    "Why": "Code-centric, ideal for software Q&A and snippets"
  },
  "phi3:3.8b": {
    "RAM": "~5-6 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… always safe",
    "Why": "Insanely efficient for size, very fast"
  },
  "phi3:3.8": {
    "RAM": "~5-6 GB",
    "Reasoning": "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ",
    "JetsonSafe": "âœ… always safe",
    "Why": "Insanely efficient for size"
  },

  "mxbai-embed-large:latest": {
    "RAM": "~3 GB",
    "Reasoning": "-",
    "JetsonSafe": "âœ… always safe",
    "Why": "Default embedding model, accurate for RAG indexing"
  },
  "snowflake-arctic-embed2": {
    "RAM": "~2.5 GB",
    "Reasoning": "-",
    "JetsonSafe": "âœ… always safe",
    "Why": "Arctic embedder, efficient for most Jetsons"
  },
  "nomic-embed-text": {
    "RAM": "~3 GB",
    "Reasoning": "-",
    "JetsonSafe": "âœ… always safe",
    "Why": "Nomic text embedder, open-source, general RAG"
  }
}
